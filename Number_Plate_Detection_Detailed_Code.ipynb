{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cnwtu--5qfUs"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "# For data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "# For numerical operations\n",
        "import numpy as np\n",
        "# For computer vision and image processing\n",
        "import cv2\n",
        "# For building and training machine learning models\n",
        "import tensorflow as tf\n",
        "# For evaluating model performance using F1 score\n",
        "from sklearn.metrics import f1_score\n",
        "# For optimization algorithms\n",
        "from tensorflow.keras import optimizers\n",
        "# Sequential model type for building neural networks\n",
        "from tensorflow.keras.models import Sequential\n",
        "# For image data augmentation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# Layers used to build CNNs\n",
        "from tensorflow.keras.layers import Dense, Flatten, MaxPooling2D, Dropout, Conv2D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained Haar Cascade classifier for detecting Indian license plates\n",
        "plate_cascade = cv2.CascadeClassifier('indian_license_plate.xml')"
      ],
      "metadata": {
        "id": "v23vZPXxsXOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create copies of the input image for processing and for ROI extraction\n",
        "def detect_plate(img, text=''):\n",
        "  # Copy of the original image for drawing rectangles\n",
        "    plate_img = img.copy()\n",
        "    # Copy of the original image for extracting the Region of Interest (ROI)\n",
        "    roi = img.copy()\n",
        "\n",
        "    # Detect license plates in the image using the pre-trained cascade classifier\n",
        "    plate_rect = plate_cascade.detectMultiScale(\n",
        "        plate_img,                # Image to perform detection on\n",
        "        scaleFactor=1.2,          # Scale factor for image resizing to detect plates at various scales\n",
        "        minNeighbors=7            # Minimum number of neighbors each candidate rectangle should have to retain it\n",
        "    )\n",
        "\n",
        "    # Iterate through detected plates and process them\n",
        "    for (x, y, w, h) in plate_rect:\n",
        "        # Define the Region of Interest (ROI) as the detected plate area\n",
        "        # ROI for future processing if needed\n",
        "        roi_ = roi[y:y+h, x:x+w, :]\n",
        "        # The actual plate image to return\n",
        "        plate = roi[y:y+h, x:x+w, :]\n",
        "\n",
        "        # Draw a rectangle around the detected plate in the plate_img copy\n",
        "        cv2.rectangle(\n",
        "            plate_img,              # Image on which to draw\n",
        "            (x+2, y),               # Top-left corner of the rectangle (with a small offset)\n",
        "            (x+w-3, y+h-5),         # Bottom-right corner of the rectangle (with a small offset)\n",
        "            (51, 181, 155),         # Color of the rectangle in BGR format\n",
        "            3                       # Thickness of the rectangle border\n",
        "        )\n",
        "\n",
        "    # If text is provided, overlay the text on the detected plate\n",
        "    if text != '':\n",
        "        plate_img = cv2.putText(\n",
        "            plate_img,              # Image on which to overlay text\n",
        "            text,                   # Text to overlay (e.g., detected license plate number)\n",
        "            (x - w // 2, y - h // 2), # Position for the text near the detected plate\n",
        "            cv2.FONT_HERSHEY_COMPLEX_SMALL, # Font style\n",
        "            0.5,                    # Font scale (size)\n",
        "            (51, 181, 155),         # Font color in BGR format\n",
        "            1,                      # Thickness of the text\n",
        "            cv2.LINE_AA             # Line type for smooth text\n",
        "        )\n",
        "\n",
        "    # Return the image with the drawn rectangle and the extracted plate image\n",
        "    return plate_img, plate\n"
      ],
      "metadata": {
        "id": "__8PbiHAsXr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the color of the image from BGR (OpenCV default) to RGB (Matplotlib default)\n",
        "def display(img_, title=''):\n",
        "  # Create a Matplotlib figure to display the image\n",
        "    img = cv2.cvtColor(img_, cv2.COLOR_BGR2RGB)\n",
        "    # Set the figure size to 10x6 inches\n",
        "    fig = plt.figure(figsize=(10,6))\n",
        "    # Add a subplot to the figure\n",
        "    ax = plt.subplot(111)\n",
        "    # Display the image in the subplot\n",
        "    ax.imshow(img)                     # Show the image on the subplot\n",
        "    plt.axis('off')                    # Turn off axis lines and ticks for a cleaner display\n",
        "    plt.title(title)                   # Set the title for the display window\n",
        "    plt.show()                         # Render the image on the screen\n",
        "\n",
        "# Load an image from a file\n",
        "img = cv2.imread('car.jpg')            # Read 'car.jpg' using OpenCV (loads in BGR format)\n",
        "display(img, 'input image')            # Display the image with the title 'input image'\n"
      ],
      "metadata": {
        "id": "EAIGva84sXuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect license plates in the given image and get the processed output and the extracted plate\n",
        "output_img, plate = detect_plate(img)"
      ],
      "metadata": {
        "id": "GXWy8QuysXwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the processed image with the detected license plate highlighted\n",
        "display(output_img, 'detected license plate in the input image')"
      ],
      "metadata": {
        "id": "kxIu9kH9sXyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the extracted license plate region with a descriptive title\n",
        "display(plate, 'extracted license plate from the image')"
      ],
      "metadata": {
        "id": "4z6sjSv2sX1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_contours(dimensions, img):\n",
        "    # Find contours in the binary image\n",
        "    cntrs, _ = cv2.findContours(img.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Unpack contour filtering dimensions\n",
        "    lower_width = dimensions[0]   # Minimum width of contour to consider\n",
        "    upper_width = dimensions[1]   # Maximum width of contour to consider\n",
        "    lower_height = dimensions[2]  # Minimum height of contour to consider\n",
        "    upper_height = dimensions[3]  # Maximum height of contour to consider\n",
        "\n",
        "    # Sort contours by area in descending order and keep the top 15 largest\n",
        "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
        "\n",
        "    # Load an image to visualize contours (assumes 'contour.jpg' exists)\n",
        "    ii = cv2.imread('contour.jpg')\n",
        "\n",
        "    # Lists to store x-coordinates of contours, target contours, and character images\n",
        "    x_cntr_list = []\n",
        "    target_contours = []\n",
        "    img_res = []\n",
        "\n",
        "    # Loop through each contour to find target contours based on size\n",
        "    for cntr in cntrs:\n",
        "        # Get the bounding box coordinates and dimensions of the contour\n",
        "        intX, intY, intWidth, intHeight = cv2.boundingRect(cntr)\n",
        "\n",
        "        # Check if contour falls within specified width and height ranges\n",
        "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height:\n",
        "            x_cntr_list.append(intX)  # Store x-coordinate for sorting\n",
        "\n",
        "            # Initialize a blank image for the character\n",
        "            char_copy = np.zeros((44, 24))\n",
        "\n",
        "            # Extract and resize the character from the image\n",
        "            char = img[intY:intY+intHeight, intX:intX+intWidth]  # Crop character from the image\n",
        "            char = cv2.resize(char, (20, 40))                    # Resize to standard size\n",
        "\n",
        "            # Draw rectangle around the detected contour in the loaded image for visualization\n",
        "            cv2.rectangle(ii, (intX, intY), (intWidth + intX, intY + intHeight), (50, 21, 200), 2)\n",
        "            plt.imshow(ii, cmap='gray')  # Display updated image with rectangle\n",
        "\n",
        "            # Invert colors of the character (black becomes white and vice versa)\n",
        "            char = cv2.subtract(255, char)\n",
        "\n",
        "            # Place resized character in the center of `char_copy` (44x24 size)\n",
        "            char_copy[2:42, 2:22] = char\n",
        "\n",
        "            # Set padding around the character in `char_copy` to zero for uniformity\n",
        "            char_copy[0:2, :] = 0         # Top padding\n",
        "            char_copy[:, 0:2] = 0         # Left padding\n",
        "            char_copy[42:44, :] = 0       # Bottom padding\n",
        "            char_copy[:, 22:24] = 0       # Right padding\n",
        "\n",
        "            # Append the processed character image to the result list\n",
        "            img_res.append(char_copy)\n",
        "\n",
        "    plt.show()  # Show the visualization with drawn contours\n",
        "\n",
        "    # Sort characters by x-coordinate to ensure they are in left-to-right order\n",
        "    indices = sorted(range(len(x_cntr_list)), key=lambda k: x_cntr_list[k])\n",
        "\n",
        "    # Reorder `img_res` based on sorted x-coordinates\n",
        "    img_res_copy = [img_res[idx] for idx in indices]\n",
        "    img_res = np.array(img_res_copy)\n",
        "\n",
        "    # Return the ordered character images\n",
        "    return img_res"
      ],
      "metadata": {
        "id": "LapiFH0nsX48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_characters(image):\n",
        "    # Resize the input image to a standard size suitable for character segmentation\n",
        "    img_lp = cv2.resize(image, (333, 75))  # Resize to 333x75 pixels (common license plate dimensions)\n",
        "\n",
        "    # Convert the resized image to grayscale\n",
        "    img_gray_lp = cv2.cvtColor(img_lp, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply binary thresholding using Otsu's method for optimal thresholding\n",
        "    _, img_binary_lp = cv2.threshold(img_gray_lp, 200, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Perform morphological operations to clean up the binary image\n",
        "    img_binary_lp = cv2.erode(img_binary_lp, (3,3))   # Erode to reduce noise\n",
        "    img_binary_lp = cv2.dilate(img_binary_lp, (3,3))  # Dilate to restore character structure\n",
        "\n",
        "    # Define the dimensions of the binary image\n",
        "    LP_WIDTH = img_binary_lp.shape[0]   # Width of the license plate image\n",
        "    LP_HEIGHT = img_binary_lp.shape[1]  # Height of the license plate image\n",
        "\n",
        "    # Set the outer borders of the image to white (background color)\n",
        "    img_binary_lp[0:3, :] = 255            # Top border\n",
        "    img_binary_lp[:, 0:3] = 255            # Left border\n",
        "    img_binary_lp[72:75, :] = 255          # Bottom border\n",
        "    img_binary_lp[:, 330:333] = 255        # Right border\n",
        "\n",
        "    # Define the approximate dimensions for characters within the license plate\n",
        "    dimensions = [\n",
        "        LP_WIDTH / 6,                      # Minimum width of a character\n",
        "        LP_WIDTH / 2,                      # Maximum width of a character\n",
        "        LP_HEIGHT / 10,                    # Minimum height of a character\n",
        "        2 * LP_HEIGHT / 3                  # Maximum height of a character\n",
        "    ]\n",
        "\n",
        "    # Display the processed binary image for visual verification\n",
        "    plt.imshow(img_binary_lp, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    # Save the binary image to a file (used by `find_contours` to visualize contours)\n",
        "    cv2.imwrite('contour.jpg', img_binary_lp)\n",
        "\n",
        "    # Use the `find_contours` function to detect and segment characters based on dimensions\n",
        "    char_list = find_contours(dimensions, img_binary_lp)\n",
        "\n",
        "    # Return the list of segmented character images\n",
        "    return char_list"
      ],
      "metadata": {
        "id": "nmceib9OuNTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Segment characters from the 'plate' image by calling the segment_characters function\n",
        "char = segment_characters(plate)"
      ],
      "metadata": {
        "id": "vFzY_WkQuNWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i+1)  # Create a subplot in a grid of 1 row and 10 columns, positioning it at i+1\n",
        "    plt.imshow(char[i], cmap='gray')  # Display the ith character in 'char' list using a grayscale colormap\n",
        "    plt.axis('off')  # Hide the axis labels (since we're displaying images)"
      ],
      "metadata": {
        "id": "bBzTYhn8uzBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator instance for image augmentation during training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values to the range [0, 1] (by dividing by 255)\n",
        "    rotation_range=20,  # Randomly rotate images by up to 20 degrees\n",
        "    width_shift_range=0.2,  # Randomly shift the image horizontally by 20% of the width\n",
        "    height_shift_range=0.2,  # Randomly shift the image vertically by 20% of the height\n",
        "    shear_range=0.2,  # Apply shear transformations (slanting of the image) by up to 20%\n",
        "    zoom_range=0.2,  # Randomly zoom in or out on images by up to 20%\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    fill_mode='nearest'  # Fill in newly created pixels after transformations with the nearest pixel value\n",
        ")\n",
        "\n",
        "# Create the train data generator that loads images from the 'train' directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'train',  # Directory containing the training images organized in subfolders (one per class)\n",
        "    target_size=(28, 28),  # Resize all images to 28x28 pixels\n",
        "    batch_size=32,  # Load 32 images in each batch\n",
        "    class_mode='sparse'  # Labels will be integer-encoded (i.e., the class index will be returned)\n",
        ")\n",
        "\n",
        "# Create the validation data generator that loads images from the 'val' directory\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    'val',  # Directory containing the validation images organized in subfolders (one per class)\n",
        "    target_size=(28, 28),  # Resize all images to 28x28 pixels\n",
        "    batch_size=32,  # Load 32 images in each batch\n",
        "    class_mode='sparse'  # Labels will be integer-encoded\n",
        ")"
      ],
      "metadata": {
        "id": "4UGyn-2IuzEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute the F1 score\n",
        "def f1score(y, y_pred):\n",
        "    # Compute F1 score using the 'f1_score' function from sklearn\n",
        "    return f1_score(y, tf.math.argmax(y_pred, axis=1), average='micro')\n",
        "\n",
        "# Function to compute F1 score as a TensorFlow-compatible custom metric\n",
        "def custom_f1score(y, y_pred):\n",
        "    # Use 'tf.py_function' to wrap the f1score function and make it compatible with TensorFlow's execution\n",
        "    return tf.py_function(f1score, (y, y_pred), tf.double)"
      ],
      "metadata": {
        "id": "41KhlGTvuzG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Clear the current TensorFlow session to avoid clutter from old models\n",
        "K.clear_session()\n",
        "\n",
        "# Initialize a new Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a 2D convolutional layer with 16 filters of size 22x22\n",
        "model.add(Conv2D(16, (22, 22), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
        "# - 16 filters (kernels) are applied to the input image (28x28x3).\n",
        "# - The filter size is 22x22.\n",
        "# - 'relu' is used as the activation function, which applies a rectified linear unit transformation (max(0, x)) to each pixel.\n",
        "# - 'padding=same' ensures that the output feature map has the same width and height as the input by padding the borders.\n",
        "# - `input_shape=(28, 28, 3)` specifies the shape of the input image (28x28 pixels with 3 color channels).\n",
        "\n",
        "# Add another 2D convolutional layer with 32 filters of size 16x16\n",
        "model.add(Conv2D(32, (16, 16), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
        "# - This layer uses 32 filters of size 16x16.\n",
        "# - 'relu' activation function is applied again.\n",
        "\n",
        "# Add a third convolutional layer with 64 filters of size 8x8\n",
        "model.add(Conv2D(64, (8, 8), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
        "# - 64 filters of size 8x8 are applied here.\n",
        "\n",
        "# Add a fourth convolutional layer with 64 filters of size 4x4\n",
        "model.add(Conv2D(64, (4, 4), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
        "# - 64 filters of size 4x4 are applied.\n",
        "\n",
        "# Add a max pooling layer with a pool size of 4x4\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "# - Max pooling is applied to reduce the spatial dimensions (width and height).\n",
        "# - A pool size of 4x4 means it takes the maximum value from each 4x4 region of the feature map.\n",
        "\n",
        "# Flatten the 2D output into a 1D array (required for the fully connected layers)\n",
        "model.add(Flatten())\n",
        "# - The `Flatten` layer converts the 2D matrix of feature maps into a 1D vector, which can then be fed into the fully connected layers.\n",
        "\n",
        "# Add a fully connected (dense) layer with 128 neurons and 'relu' activation function\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# - This layer has 128 neurons (fully connected to the flattened input).\n",
        "# - The 'relu' activation function is used again.\n",
        "\n",
        "# Add the final output layer with 36 neurons and 'softmax' activation function\n",
        "model.add(Dense(36, activation='softmax'))\n",
        "# - The final layer has 36 neurons, one for each possible class (e.g., 36 license plate characters or other classification tasks).\n",
        "# - The 'softmax' activation function is used, which outputs a probability distribution over the 36 classes. It ensures that all output values sum up to 1, which is necessary for classification tasks.\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification with integer labels\n",
        "    optimizer=optimizers.Adam(lr=0.0001),  # Adam optimizer with a learning rate of 0.0001\n",
        "    metrics=[custom_f1score]  # Custom F1 score function is used to evaluate the model during training and evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "hSDcuGtKuzJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The model.summary() function in Keras provides a summary of the model architecture, including details about the layers,\n",
        "#their output shapes, and the number of parameters (weights) in each layer.\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qYIkIpSEuzME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom callback class by subclassing `tf.keras.callbacks.Callback`\n",
        "class stop_training_callback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    # This method is called at the end of every epoch during training\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "        # Check if the custom F1 score ('val_custom_f1score') is greater than 0.99\n",
        "        if logs.get('val_custom_f1score') > 0.99:\n",
        "\n",
        "            # If the condition is met, stop training by setting `stop_training` to True\n",
        "            self.model.stop_training = True"
      ],
      "metadata": {
        "id": "VG_o8Jm5uzOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size used for training\n",
        "batch_size = 32\n",
        "\n",
        "# Instantiate the stop_training_callback, which will be used to monitor the validation F1 score\n",
        "callbacks = [stop_training_callback()]\n",
        "\n",
        "# Train the model using the .fit() method\n",
        "model.fit(\n",
        "    train_generator,  # The training data generator, which yields batches of data\n",
        "    steps_per_epoch=train_generator.samples // batch_size,  # Number of steps per epoch, calculated based on the total number of training samples and batch size\n",
        "    validation_data=validation_generator,  # The validation data generator, which provides validation data at the end of each epoch\n",
        "    epochs=15,  # Number of training epochs\n",
        "    verbose=1,  # Print progress messages during training (1 means progress bar)\n",
        "    callbacks=callbacks  # List of callbacks to be applied during training (in this case, the stop_training_callback)\n",
        ")"
      ],
      "metadata": {
        "id": "EFDtBDJzuzQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fix the image dimension to (28,28,3) by repeating the grayscale image for 3 channels\n",
        "def fix_dimension(img):\n",
        "    # Create a new empty image with 3 channels (28x28x3)\n",
        "    new_img = np.zeros((28,28,3))\n",
        "\n",
        "    # Loop over the 3 channels (RGB), and fill the channels with the same image\n",
        "    for i in range(3):\n",
        "        new_img[:,:,i] = img  # Copy the grayscale image into each of the three channels\n",
        "\n",
        "    return new_img  # Return the newly created image with 3 channels\n",
        "\n",
        "# Function to predict the plate number based on segmented characters\n",
        "def show_results():\n",
        "    dic = {}  # Dictionary to map the predicted output index to the actual character\n",
        "    characters = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'  # List of all possible characters (digits and uppercase letters)\n",
        "\n",
        "    # Create the dictionary to map index to characters\n",
        "    for i, c in enumerate(characters):\n",
        "        dic[i] = c\n",
        "\n",
        "    output = []  # List to store the predicted characters\n",
        "\n",
        "    # Loop over each character in the `char` list (which contains the segmented characters)\n",
        "    for i, ch in enumerate(char):\n",
        "        # Resize each character image to the size (28x28) using nearest-neighbor interpolation\n",
        "        img_ = cv2.resize(ch, (28, 28), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Fix the image dimension by converting it to 3 channels (RGB)\n",
        "        img = fix_dimension(img_)\n",
        "\n",
        "        # Reshape the image to match the input shape of the model (1, 28, 28, 3)\n",
        "        img = img.reshape(1, 28, 28, 3)\n",
        "\n",
        "        # Predict the class of the character using the trained model and find the most likely class\n",
        "        y_ = np.argmax(model.predict(img), axis=-1)[0]\n",
        "\n",
        "        # Get the corresponding character from the dictionary based on the predicted index\n",
        "        character = dic[y_]\n",
        "\n",
        "        # Append the predicted character to the output list\n",
        "        output.append(character)\n",
        "\n",
        "    # Join all the predicted characters into a single string representing the plate number\n",
        "    plate_number = ''.join(output)\n",
        "\n",
        "    return plate_number  # Return the predicted plate number\n",
        "\n",
        "# Call the function and print the predicted plate number\n",
        "print(show_results())"
      ],
      "metadata": {
        "id": "YdAIcr5rwElI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Segmented characters and their predicted value.\n",
        "plt.figure(figsize=(10, 6))  # Create a figure for plotting with a size of 10x6 inches\n",
        "\n",
        "# Loop over each character in the `char` list\n",
        "for i, ch in enumerate(char):\n",
        "    # Resize the character image to 28x28 pixels using interpolation method INTER_AREA\n",
        "    img = cv2.resize(ch, (28, 28), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Create a subplot for each character. The layout is a 3x4 grid, and `i+1` indicates the position of the character.\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "\n",
        "    # Display the resized character image in grayscale\n",
        "    plt.imshow(img, cmap='gray')\n",
        "\n",
        "    # Add the predicted character (from the `show_results()` function) as the title of the subplot\n",
        "    plt.title(f'predicted: {show_results()[i]}')\n",
        "\n",
        "    # Remove the axes for a cleaner presentation\n",
        "    plt.axis('off')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_queLSGGwM0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the `show_results()` function to get the predicted license plate number\n",
        "plate_number = show_results()"
      ],
      "metadata": {
        "id": "78U-wSgNwM2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vOsYEHYwM6A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}